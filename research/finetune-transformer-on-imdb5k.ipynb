{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a Transformer-based architecture on the IMDB Movie Reviews dataset for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies, save requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /opt/conda/lib/python3.6/site-packages (1.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.16.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas tqdm\n",
    "!pip install -U torch \n",
    "!pip install -q pytorch_transformers pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "tqdm\n",
    "torch==1.1.0\n",
    "pytorch_transformers\n",
    "pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# text and label column names\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# path to data \n",
    "DATA_DIR = os.path.abspath('./data')\n",
    "\n",
    "# path to IMDB\n",
    "IMDB_DIR = os.path.join(DATA_DIR, \"imdb5k\")\n",
    "\n",
    "# url to dataset\n",
    "IMDB_URL = \"https://github.com/ben0it8/transformer-finetuning/raw/master/imdb5k.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     3,
     33
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "def download_url(url:str, dest:str, overwrite:bool=True, show_progress=True, \n",
    "                 chunk_size=1024*1024, timeout=4, retries=5)->None:\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`.\"\n",
    "    \n",
    "    dest = os.path.join(dest, os.path.basename(url))\n",
    "    if os.path.exists(dest) and not overwrite: \n",
    "        print(f\"File {dest} already exists!\")\n",
    "        return dest\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "    print(f\"Downloading {url}\")\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: \n",
    "            pbar = tqdm(range(file_size), leave=False)\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Download failed after {retries} retries.\")\n",
    "            import sys;sys.exit(1)\n",
    "        finally:\n",
    "            return dest\n",
    "        \n",
    "def untar(file_path, dest:str):\n",
    "    \"Untar `file_path` to `dest`\"\n",
    "    print(f\"Untar {os.path.basename(file_path)} to {dest}\")\n",
    "    with tarfile.open(file_path) as tf:\n",
    "        tf.extractall(path=str(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ben0it8/transformer-finetuning/raw/master/imdb5k.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5382327), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Untar imdb5k.tar.gz to /workspace/data\n"
     ]
    }
   ],
   "source": [
    "# download imdb dataset\n",
    "file_path = download_url(IMDB_URL, '/tmp', overwrite=True)\n",
    "\n",
    "# untar imdb dataset to DATA_DIR\n",
    "untar(file_path, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13M\r\n",
      "-rw-r--r--. 1 501 staff 6.4M Jul 17 16:01 imdb5k_test.csv\r\n",
      "-rw-r--r--. 1 501 staff 6.4M Jul 17 16:01 imdb5k_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $IMDB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_html(raw: str):\n",
    "    \"remove html tags and whitespaces\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "def read_imdb(data_dir, max_lengths={\"train\": None, \"test\": None}):\n",
    "    datasets = {}\n",
    "    for t in [\"train\", \"test\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, f\"imdb5k_{t}.csv\"))\n",
    "        if max_lengths.get(t) is not None:\n",
    "            df = df.sample(n=max_lengths.get(t))\n",
    "            df[TEXT_COL] = df[TEXT_COL].apply(lambda t: clean_html(t))\n",
    "        datasets[t] = df\n",
    "    return datasets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read data, 5000-5000 each\n",
    "datasets = read_imdb(IMDB_DIR, max_lengths={\"train\": None, \"test\": None})\n",
    "\n",
    "# list of labels\n",
    "labels = list(set(datasets[\"train\"][LABEL_COL].tolist()))\n",
    "\n",
    "# labels to integers mapping\n",
    "label2int = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>Bah. Another tired, desultory reworking of an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>Twist endings can be really cool in a movie. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Every time I watch this movie I am more impres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>I absolutely LOVED this film! I do not at all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Though the plot elements to \"The Eighth Day\" s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   neg  Bah. Another tired, desultory reworking of an ...\n",
       "1   neg  Twist endings can be really cool in a movie. I...\n",
       "2   pos  Every time I watch this movie I am more impres...\n",
       "3   pos  I absolutely LOVED this film! I do not at all ...\n",
       "4   pos  Though the plot elements to \"The Eighth Day\" s..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "NUM_MAX_POSITIONS = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class TextProcessor:\n",
    "    \n",
    "    # special tokens for classification and padding\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    \n",
    "    def __init__(self, tokenizer, label2id: dict, num_max_positions:int=512):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "        self.num_max_positions = num_max_positions\n",
    "        \n",
    "    \n",
    "    def process_example(self, example: Tuple[str, str]):\n",
    "        \"Convert text (example[0]) to sequence of IDs and label (example[1] to integer\"\n",
    "        assert len(example) == 2\n",
    "        label, text = example[0], example[1]\n",
    "        assert isinstance(text, str)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(tokens) >= self.num_max_positions:\n",
    "            tokens = tokens[:self.num_max_positions-1] \n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]]\n",
    "        # pad if too short\n",
    "        else:\n",
    "            pad = [self.tokenizer.vocab[self.PAD]] * (self.num_max_positions-len(tokens)-1)\n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]] + pad\n",
    "        \n",
    "        return np.array(ids, dtype='int64'), self.label2id[label]\n",
    "    \n",
    "# download the 'bert-base-cased' tokenizer\n",
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "# initialize a TextProcessor\n",
    "processor = TextProcessor(tokenizer, label2int, num_max_positions=NUM_MAX_POSITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningConfig(num_classes=2, dropout=0.1, init_range=0.02, batch_size=32, lr=6.5e-05, max_norm=1.0, n_epochs=2, n_warmup=10, valid_pct=0.1, gradient_acc_steps=5, device=device(type='cuda', index=0), log_dir='./logs/')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "LOG_DIR = \"./logs/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "FineTuningConfig = namedtuple('FineTuningConfig',\n",
    "      field_names=\"num_classes, dropout, init_range, batch_size, lr, max_norm, n_epochs,\"\n",
    "                  \"n_warmup, valid_pct, gradient_acc_steps, device, log_dir\")\n",
    "\n",
    "finetuning_config = FineTuningConfig(\n",
    "                2, 0.1, 0.02, BATCH_SIZE, 6.5e-5, 1.0, 2,\n",
    "                10, 0.1, 5, device, LOG_DIR)\n",
    "\n",
    "finetuning_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from itertools import repeat\n",
    "\n",
    "num_cores = cpu_count()\n",
    "\n",
    "def process_row(processor, row):\n",
    "    return processor.process_example((row[1][LABEL_COL], row[1][TEXT_COL]))\n",
    "\n",
    "def create_dataloader(df: pd.DataFrame,\n",
    "                      processor: TextProcessor,\n",
    "                      batch_size: int = 32,\n",
    "                      shuffle: bool = False,\n",
    "                      valid_pct: float = None,\n",
    "                      text_col: str = \"text\",\n",
    "                      label_col: str = \"label\"):\n",
    "    \"Process rows in `df` with `processor` and return a  DataLoader\"\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "        result = list(\n",
    "            tqdm(executor.map(process_row,\n",
    "                              repeat(processor),\n",
    "                              df.iterrows(),\n",
    "                              chunksize=len(df) // 10),\n",
    "                 desc=f\"Processing {len(df)} examples on {num_cores} cores\",\n",
    "                 total=len(df)))\n",
    "\n",
    "    features = [r[0] for r in result]\n",
    "    labels = [r[1] for r in result]\n",
    "\n",
    "    dataset = TensorDataset(torch.tensor(features, dtype=torch.long),\n",
    "                            torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "    if valid_pct is not None:\n",
    "        valid_size = int(valid_pct * len(df))\n",
    "        train_size = len(df) - valid_size\n",
    "        valid_dataset, train_dataset = random_split(dataset,\n",
    "                                                    [valid_size, train_size])\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=0,\n",
    "                             shuffle=shuffle,\n",
    "                             pin_memory=torch.cuda.is_available())\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3074574f7ac4ec09fac8317b2e4f68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Processing 5000 examples on 80 cores', max=5000, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30dd7de6ac14ef4b9b173053f55c0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Processing 5000 examples on 80 cores', max=5000, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create train and valid sets by splitting\n",
    "train_dl, valid_dl = create_dataloader(datasets[\"train\"], processor, \n",
    "                                    batch_size=finetuning_config.batch_size, \n",
    "                                    valid_pct=finetuning_config.valid_pct)\n",
    "\n",
    "test_dl = create_dataloader(datasets[\"test\"], processor, \n",
    "                             batch_size=finetuning_config.batch_size, \n",
    "                             valid_pct=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerWithClfHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     6,
     52
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_num_params(model):\n",
    "    mp = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum(np.prod(p.size()) for p in mp)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.ReLU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h\n",
    "\n",
    "\n",
    "class TransformerWithClfHead(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                       fine_tuning_config.dropout, causal=not config.mlm)\n",
    "        \n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters discarded from the pretrained model: ['lm_head.weight']\n",
      "Parameters added in the model: ['classification_head.weight', 'classification_head.bias']\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import cached_path\n",
    "\n",
    "# download pre-trained model and config\n",
    "state_dict = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                    \"naacl-2019-tutorial/model_checkpoint.pth\"), map_location='cpu')\n",
    "\n",
    "config = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                        \"naacl-2019-tutorial/model_training_args.bin\"))\n",
    "\n",
    "# init model: Transformer base + classifier head\n",
    "model = TransformerWithClfHead(config=config, fine_tuning_config=finetuning_config).to(finetuning_config.device)\n",
    "\n",
    "incompatible_keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Parameters discarded from the pretrained model: {incompatible_keys.unexpected_keys}\")\n",
    "print(f\"Parameters added in the model: {incompatible_keys.missing_keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare fine-tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     38
    ]
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy \n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers.optimization import AdamW\n",
    "\n",
    "# Bert optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=finetuning_config.lr, correct_bias=False) \n",
    "\n",
    "def update(engine, batch):\n",
    "    \"update function for training\"\n",
    "    model.train()\n",
    "    inputs, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "    inputs = inputs.transpose(0, 1).contiguous() # [S, B]\n",
    "    _, loss = model(inputs, \n",
    "                    clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]), \n",
    "                    clf_labels=labels)\n",
    "    loss = loss / finetuning_config.gradient_acc_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), finetuning_config.max_norm)\n",
    "    if engine.state.iteration % finetuning_config.gradient_acc_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def inference(engine, batch):\n",
    "    \"update function for evaluation\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "        inputs = batch.transpose(0, 1).contiguous()\n",
    "        logits = model(inputs,\n",
    "                       clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]),\n",
    "                       padding_mask = (batch == tokenizer.vocab[processor.PAD]))\n",
    "    return logits, labels\n",
    "\n",
    "def predict(model, tokenizer, int2label, input=\"test\"):\n",
    "    \"predict `input` with `model`\"\n",
    "    tok = tokenizer.tokenize(input)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tok) + [tokenizer.vocab['[CLS]']]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor.transpose(0, 1).contiguous() # [S, 1]\n",
    "    logits = model(tensor_in,\n",
    "                   clf_tokens_mask = (tensor_in == tokenizer.vocab['[CLS]']),\n",
    "                   padding_mask = (tensor == tokenizer.vocab['[PAD]']))\n",
    "    val, _ = torch.max(logits, 0)\n",
    "    val = F.softmax(val, dim=0).detach().cpu().numpy()    \n",
    "    return {int2label[val.argmax()]: val.max(),\n",
    "            int2label[val.argmin()]: val.min()}\n",
    "trainer = Engine(update)\n",
    "evaluator = Engine(inference)\n",
    "\n",
    "# add metric to evaluator \n",
    "Accuracy().attach(evaluator, \"accuracy\")\n",
    "\n",
    "# add evaluator to trainer: eval on valid set after each epoch\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(valid_dl)\n",
    "    print(f\"validation epoch: {engine.state.epoch} acc: {100*evaluator.state.metrics['accuracy']}\")\n",
    "          \n",
    "# lr schedule: linearly warm-up to lr and then to zero\n",
    "scheduler = PiecewiseLinear(optimizer, 'lr', [(0, 0.0), (finetuning_config.n_warmup, finetuning_config.lr),\n",
    "                                              (len(train_dl)*finetuning_config.n_epochs, 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "\n",
    "# add progressbar with loss\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "ProgressBar(persist=True).attach(trainer, metric_names=['loss'])\n",
    "\n",
    "# save checkpoints and finetuning config\n",
    "checkpoint_handler = ModelCheckpoint(finetuning_config.log_dir, 'finetuning_checkpoint', \n",
    "                                     save_interval=1, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'imdb_model': model})\n",
    "\n",
    "int2label = {i:label for label,i in label2int.items()}\n",
    "\n",
    "# save metadata\n",
    "torch.save({\n",
    "    \"config\": config,\n",
    "    \"config_ft\": finetuning_config,\n",
    "    \"int2label\": int2label\n",
    "}, os.path.join(finetuning_config.log_dir, \"metadata.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets fine-tune on imdb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc5f502b1d845cc910451318a67b595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 acc: 76.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43dc57737f6406cbe632a0ba2bb19e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 2 acc: 87.0\n",
      "\n",
      "test results - acc: 87.280\n"
     ]
    }
   ],
   "source": [
    "# fit the model on train_dl\n",
    "trainer.run(train_dl, max_epochs=finetuning_config.n_epochs)\n",
    "\n",
    "# evaluate the model on test_dl\n",
    "evaluator.run(test_dl)\n",
    "print(f\"test results - acc: {100*evaluator.state.metrics['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 590728\r\n",
      "-rw-r--r--. 1 root root       318 Jul 25 11:53 fine_tuning_args.bin\r\n",
      "-rw-------. 1 root root 201630224 Jul 26 09:47 finetuning_checkpoint_imdb_model_2.pth\r\n",
      "-rw-------. 1 root root 201630224 Jul 26 09:14 finetuning_checkpoint_imdb_model_5.pth\r\n",
      "-rw-r--r--. 1 root root      1074 Jul 26 09:44 metadata.bin\r\n",
      "-rw-r--r--. 1 root root 201627491 Jul 23 09:58 model_weights.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l $finetuning_config.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "torch.save(model.state_dict(), os.path.join(finetuning_config.log_dir, \"model_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 0.9364333, 'neg': 0.06356672}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, tokenizer, int2label, input = \"I just love how the actors are playing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.68089634, 'pos': 0.3191037}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, tokenizer, int2label, input = \"This movie is poorly directed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\"\"\"utils.py: Utility code for Transformer fine-tuning on the IMDB Movie Reviews dataset.\"\"\"\n",
    "\n",
    "__author__ = \"Oliver Atanaszov\"\n",
    "__email__ = \"oliver.atanaszov@gmail.com\"\n",
    "__github__ = \"https://github.com/ben0it8\"\n",
    "__copyright__ = \"Copyright 2019, Planet Earth\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from itertools import repeat\n",
    "from datetime import timedelta\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, cached_path, AdamW\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy, ConfusionMatrix\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "\n",
    "logger = logging.getLogger(\"utils.py\")\n",
    "\n",
    "num_cores = cpu_count()\n",
    "\n",
    "# text and label column names\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "FineTuningConfig = namedtuple(\n",
    "    'FineTuningConfig',\n",
    "    field_names=\n",
    "    \"num_classes, dropout, init_range, batch_size, lr, max_norm, n_epochs,\"\n",
    "    \"n_warmup, valid_pct, gradient_acc_steps, device, log_dir\")\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "\n",
    "    # special tokens for classification and padding\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "\n",
    "    def __init__(self, tokenizer, label2id: dict,\n",
    "                 num_max_positions: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "        self.num_max_positions = num_max_positions\n",
    "\n",
    "    def process_example(self, example: Tuple[str, str]):\n",
    "        \"Convert text (example[0]) to sequence of IDs and label (example[1] to integer\"\n",
    "        assert len(example) == 2\n",
    "        label, text = example[0], example[1]\n",
    "        assert isinstance(text, str)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(tokens) >= self.num_max_positions:\n",
    "            tokens = tokens[:self.num_max_positions - 1]\n",
    "            ids = self.tokenizer.convert_tokens_to_ids(tokens) + [\n",
    "                self.tokenizer.vocab[self.CLS]\n",
    "            ]\n",
    "        # pad if too short\n",
    "        else:\n",
    "            pad = [self.tokenizer.vocab[self.PAD]\n",
    "                   ] * (self.num_max_positions - len(tokens) - 1)\n",
    "            ids = self.tokenizer.convert_tokens_to_ids(tokens) + [\n",
    "                self.tokenizer.vocab[self.CLS]\n",
    "            ] + pad\n",
    "\n",
    "        return ids, self.label2id[label]\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings,\n",
    "                 num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(\n",
    "        ), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(\n",
    "                nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(\n",
    "                nn.Sequential(nn.Linear(embed_dim, hidden_dim), nn.ReLU(),\n",
    "                              nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)),\n",
    "                                   -float('Inf'),\n",
    "                                   device=h.device,\n",
    "                                   dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(\n",
    "                self.layer_norms_1, self.attentions, self.layer_norms_2,\n",
    "                self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h,\n",
    "                             h,\n",
    "                             h,\n",
    "                             attn_mask=attn_mask,\n",
    "                             need_weights=False,\n",
    "                             key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h\n",
    "\n",
    "\n",
    "class TransformerWithClfHead(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim,\n",
    "                                       config.hidden_dim,\n",
    "                                       config.num_embeddings,\n",
    "                                       config.num_max_positions,\n",
    "                                       config.num_heads,\n",
    "                                       config.num_layers,\n",
    "                                       fine_tuning_config.dropout,\n",
    "                                       causal=not config.mlm)\n",
    "\n",
    "        self.classification_head = nn.Linear(config.embed_dim,\n",
    "                                             fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module,\n",
    "                      (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states *\n",
    "                             clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)),\n",
    "                            clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "        return clf_logits\n",
    "\n",
    "\n",
    "def getenv_cast(var: str, cast=None):\n",
    "    value = os.getenv(var)\n",
    "    if value is not None and cast is not None:\n",
    "        value = cast(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def freeze_body(model):\n",
    "    for idx, (_, child) in enumerate(model.named_children()):\n",
    "        if idx == 0:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            freeze_body(child)\n",
    "\n",
    "\n",
    "def timeit(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        ts = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        delta = timedelta(seconds=round(time() - ts, 1))\n",
    "        print(f\"Elapsed time for {f.__name__}: {str(delta):0>8}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def download_url(url: str,\n",
    "                 dest: str,\n",
    "                 overwrite: bool = True,\n",
    "                 show_progress=True,\n",
    "                 chunk_size=1024 * 1024,\n",
    "                 timeout=4,\n",
    "                 retries=5) -> None:\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`.\"\n",
    "    dest = os.path.join(dest, os.path.basename(url))\n",
    "    if os.path.exists(dest) and not overwrite:\n",
    "        logger.warning(f\"File {dest} already exists!\")\n",
    "        return dest\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://', requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try:\n",
    "        file_size = int(u.headers[\"Content-Length\"])\n",
    "    except:\n",
    "        show_progress = False\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress:\n",
    "            pbar = tqdm(range(file_size),\n",
    "                        leave=True,\n",
    "                        desc=f\"Downloading {os.path.basename(url)}\")\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.warning(f\"Download failed after {retries} retries.\")\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "        finally:\n",
    "            return dest\n",
    "\n",
    "\n",
    "@timeit\n",
    "def untar(file_path, dest: str):\n",
    "    \"Untar `file_path` to `dest`\"\n",
    "    logger.info(f\"Untar {os.path.basename(file_path)} to {dest}\")\n",
    "    with tarfile.open(file_path) as tf:\n",
    "        tf.extractall(path=str(dest))\n",
    "\n",
    "\n",
    "def clean_html(raw: str):\n",
    "    \"remove html tags and whitespaces\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def read_imdb(data_dir, max_lengths={\"train\": None, \"test\": None}):\n",
    "    datasets = {}\n",
    "    for t in [\"train\", \"test\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, f\"imdb5k_{t}.csv\"))\n",
    "        maxlen = max_lengths.get(t)\n",
    "        if maxlen is not None and maxlen <= len(df):\n",
    "            df = df.sample(n=maxlen)\n",
    "        df[TEXT_COL] = df[TEXT_COL].apply(lambda t: clean_html(t))\n",
    "        datasets[t] = df\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def process_row(processor, row):\n",
    "    return processor.process_example((row[1][LABEL_COL], row[1][TEXT_COL]))\n",
    "\n",
    "\n",
    "def create_dataloader(df: pd.DataFrame,\n",
    "                      processor: TextProcessor,\n",
    "                      batch_size: int = 32,\n",
    "                      shuffle: bool = False,\n",
    "                      valid_pct: float = None,\n",
    "                      text_col: str = \"text\",\n",
    "                      label_col: str = \"label\"):\n",
    "    \"Process rows in `df` with `processor` and return a  DataLoader\"\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "        result = list(\n",
    "            tqdm(executor.map(process_row,\n",
    "                              repeat(processor),\n",
    "                              df.iterrows(),\n",
    "                              chunksize=len(df) // 10),\n",
    "                 desc=f\"Processing {len(df)} examples on {num_cores} cores\",\n",
    "                 total=len(df)))\n",
    "\n",
    "    features = [r[0] for r in result]\n",
    "    labels = [r[1] for r in result]\n",
    "\n",
    "    dataset = TensorDataset(torch.tensor(features, dtype=torch.long),\n",
    "                            torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "    if valid_pct is not None:\n",
    "        valid_size = int(valid_pct * len(df))\n",
    "        train_size = len(df) - valid_size\n",
    "        valid_dataset, train_dataset = random_split(dataset,\n",
    "                                                    [valid_size, train_size])\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=0,\n",
    "                             shuffle=shuffle,\n",
    "                             pin_memory=torch.cuda.is_available())\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, int2label, device=None, input=\"test\"):\n",
    "    \"predict `input` with `model`\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    tok = tokenizer.tokenize(input)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tok) + [tokenizer.vocab['[CLS]']]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor.transpose(0, 1).contiguous()  # [S, 1]\n",
    "    logits = model(tensor_in,\n",
    "                   clf_tokens_mask=(tensor_in == tokenizer.vocab['[CLS]']),\n",
    "                   padding_mask=(tensor == tokenizer.vocab['[PAD]']))\n",
    "    val, _ = torch.max(logits, 0)\n",
    "    val = F.softmax(val, dim=0).detach().cpu().numpy()\n",
    "    return {\n",
    "        int2label[val.argmax()]: val.max(),\n",
    "        int2label[val.argmin()]: val.min()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottle webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bottle\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/d1/efdd0a5584169cdf791d726264089ce5d96846a8978c44ac6e13ae234327/bottle-0.12.17-py3-none-any.whl (89kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 4.9MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: bottle\n",
      "Successfully installed bottle-0.12.17\n"
     ]
    }
   ],
   "source": [
    "!pip install bottle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_config.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bottle v0.12.16 server starting up (using WSGIRefServer())...\n",
      "Listening on http://0.0.0.0:5000/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python3\n",
    "from bottle import Bottle, run, request, response\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from json import dumps\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from utils import TransformerWithClfHead, FineTuningConfig, predict\n",
    "\n",
    "logger = logging.getLogger(\"app.py\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased',\n",
    "                                          do_lower_case=False)\n",
    "\n",
    "metadata = torch.load(\"./logs/metadata.bin\")\n",
    "state_dict = torch.load(\"./logs/model_weights.pth\", map_location=device)\n",
    "\n",
    "model = TransformerWithClfHead(metadata[\"config\"], metadata[\"config_ft\"])\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "app = Bottle()\n",
    "\n",
    "@app.route(\"/inference\")\n",
    "def inference():\n",
    "    return '''\n",
    "        <form action=\"/inference\" method=\"post\">\n",
    "            Text: <input name=\"text\" type=\"text\" />\n",
    "            <input value=\"Inference\" type=\"submit\" />\n",
    "        </form>\n",
    "        '''\n",
    "\n",
    "@app.route(\"/inference\", method='POST')\n",
    "def do_inference():\n",
    "    text = request.forms.get(\"text\")\n",
    "    print(f\"text: {text}\")\n",
    "    output = predict(model,\n",
    "                     tokenizer,\n",
    "                     metadata['int2label'],\n",
    "                     device=device,\n",
    "                     input=text)\n",
    "    print(output)\n",
    "    response.content_type = \"application/json\"\n",
    "    return dumps({k: str(v) for k, v in output.items()}, indent=4)\n",
    "\n",
    "logger.info(f\"Web app for movie sentiment analysis started. Inference device: {device}\")\n",
    "run(app, host=\"0.0.0.0\", port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH gpu-remote Py 3.6",
   "language": "",
   "name": "rik_ssh_gpu_remote_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
